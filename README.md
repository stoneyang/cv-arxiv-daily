[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2022.09.17

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#pretrain>pretrain</a></li>
    <li><a href=#downstream>downstream</a></li>
    <li><a href=#adaptor>adaptor</a></li>
    <li><a href=#object-detection>object detection</a></li>
  </ol>
</details>

## pretrain

|Publish Date|Title|Authors|PDF|Code|
|---|---|---|---|---|
|**2022-09-15**|**OmniVL:One Foundation Model for Image-Language and Video-Language Tasks**|Junke Wang et.al.|[2209.07526v1](http://arxiv.org/abs/2209.07526v1)|null|
|**2022-09-15**|**Compositional generalization through abstract representations in human and artificial neural networks**|Takuya Ito et.al.|[2209.07431v1](http://arxiv.org/abs/2209.07431v1)|null|
|**2022-09-15**|**ÚFAL CorPipe at CRAC 2022: Effectivity of Multilingual Models for Coreference Resolution**|Milan Straka et.al.|[2209.07278v1](http://arxiv.org/abs/2209.07278v1)|**[link](https://github.com/ufal/crac2022-corpipe)**|
|**2022-09-15**|**uChecker: Masked Pretrained Language Models as Unsupervised Chinese Spelling Checkers**|Piji Li et.al.|[2209.07068v1](http://arxiv.org/abs/2209.07068v1)|null|
|**2022-09-15**|**VIPHY: Probing "Visible" Physical Commonsense Knowledge**|Shikhar Singh et.al.|[2209.07000v1](http://arxiv.org/abs/2209.07000v1)|**[link](https://github.com/axe--/viphy)**|
|**2022-09-15**|**Non-Parallel Voice Conversion for ASR Augmentation**|Gary Wang et.al.|[2209.06987v1](http://arxiv.org/abs/2209.06987v1)|null|
|**2022-09-14**|**PointACL:Adversarial Contrastive Learning for Robust Point Clouds Representation under Adversarial Attack**|Junxuan Huang et.al.|[2209.06971v1](http://arxiv.org/abs/2209.06971v1)|null|
|**2022-09-14**|**Finetuning Pretrained Vision-Language Models with Correlation Information Bottleneck for Robust Visual Question Answering**|Jingjing Jiang et.al.|[2209.06954v1](http://arxiv.org/abs/2209.06954v1)|null|
|**2022-09-14**|**PaLI: A Jointly-Scaled Multilingual Language-Image Model**|Xi Chen et.al.|[2209.06794v1](http://arxiv.org/abs/2209.06794v1)|null|
|**2022-09-14**|**Drawing Causal Inferences About Performance Effects in NLP**|Sandra Wankmüller et.al.|[2209.06790v1](http://arxiv.org/abs/2209.06790v1)|null|
|**2022-09-14**|**CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment**|Hongwei Xue et.al.|[2209.06430v1](http://arxiv.org/abs/2209.06430v1)|**[link](https://github.com/microsoft/xpretrain)**|
|**2022-09-13**|**CometKiwi: IST-Unbabel 2022 Submission for the Quality Estimation Shared Task**|Ricardo Rei et.al.|[2209.06243v1](http://arxiv.org/abs/2209.06243v1)|null|
|**2022-09-13**|**StoryDALL-E: Adapting Pretrained Text-to-Image Transformers for Story Continuation**|Adyasha Maharana et.al.|[2209.06192v1](http://arxiv.org/abs/2209.06192v1)|**[link](https://github.com/adymaharana/storydalle)**|
|**2022-09-13**|**Automated classification for open-ended questions with BERT**|Hyukjun Gweon et.al.|[2209.06178v1](http://arxiv.org/abs/2209.06178v1)|null|
|**2022-09-12**|**VL-Taboo: An Analysis of Attribute-based Zero-shot Capabilities of Vision-Language Models**|Felix Vogel et.al.|[2209.06103v1](http://arxiv.org/abs/2209.06103v1)|null|
|**2022-09-13**|**SeRP: Self-Supervised Representation Learning Using Perturbed Point Clouds**|Siddhant Garg et.al.|[2209.06067v1](http://arxiv.org/abs/2209.06067v1)|null|
|**2022-09-13**|**Neural3Points: Learning to Generate Physically Realistic Full-body Motion for Virtual Reality Users**|Yongjing Ye et.al.|[2209.05753v1](http://arxiv.org/abs/2209.05753v1)|null|
|**2022-09-12**|**Polycrystal Graph Neural Network**|Minyi Dai et.al.|[2209.05583v1](http://arxiv.org/abs/2209.05583v1)|**[link](https://github.com/mdai26/pgnn)**|
|**2022-09-12**|**A Molecular Multimodal Foundation Model Associating Molecule Graphs with Natural Language**|Bing Su et.al.|[2209.05481v1](http://arxiv.org/abs/2209.05481v1)|null|
|**2022-09-11**|**Learning When to Say "I Don't Know"**|Nicholas Kashani Motlagh et.al.|[2209.04944v1](http://arxiv.org/abs/2209.04944v1)|**[link](https://github.com/osu-cvl/learning-idk)**|
|**2022-09-11**|**Learning to diagnose common thorax diseases on chest radiographs from radiology reports in Vietnamese**|Thao T. B. Nguyen et.al.|[2209.04794v1](http://arxiv.org/abs/2209.04794v1)|null|
|**2022-09-10**|**Simple and Effective Gradient-Based Tuning of Sequence-to-Sequence Models**|Jared Lichtarge et.al.|[2209.04683v1](http://arxiv.org/abs/2209.04683v1)|null|

<p align=right>(<a href=#Updated-on-20220917>back to top</a>)</p>

## downstream

|Publish Date|Title|Authors|PDF|Code|
|---|---|---|---|---|
|**2022-09-15**|**OmniVL:One Foundation Model for Image-Language and Video-Language Tasks**|Junke Wang et.al.|[2209.07526v1](http://arxiv.org/abs/2209.07526v1)|null|
|**2022-09-15**|**Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models**|Manli Shu et.al.|[2209.07511v1](http://arxiv.org/abs/2209.07511v1)|null|
|**2022-09-15**|**MVNet: Memory Assistance and Vocal Reinforcement Network for Speech Enhancement**|Jianrong Wang et.al.|[2209.07302v1](http://arxiv.org/abs/2209.07302v1)|null|
|**2022-09-15**|**BadRes: Reveal the Backdoors through Residual Connection**|Mingrui He et.al.|[2209.07125v1](http://arxiv.org/abs/2209.07125v1)|null|
|**2022-09-15**|**Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge**|Zhihong Chen et.al.|[2209.07118v1](http://arxiv.org/abs/2209.07118v1)|null|
|**2022-09-15**|**Multi-Modal Masked Autoencoders for Medical Vision-and-Language Pre-Training**|Zhihong Chen et.al.|[2209.07098v1](http://arxiv.org/abs/2209.07098v1)|**[link](https://github.com/zhjohnchan/m3ae)**|
|**2022-09-15**|**Gromov-Wasserstein Autoencoders**|Nao Nakagawa et.al.|[2209.07007v1](http://arxiv.org/abs/2209.07007v1)|null|
|**2022-09-14**|**PointACL:Adversarial Contrastive Learning for Robust Point Clouds Representation under Adversarial Attack**|Junxuan Huang et.al.|[2209.06971v1](http://arxiv.org/abs/2209.06971v1)|null|
|**2022-09-14**|**Efficient multi-relational network representation using primes**|Konstantinos Bougiatiotis et.al.|[2209.06575v1](http://arxiv.org/abs/2209.06575v1)|null|
|**2022-09-15**|**Learning to Evaluate Performance of Multi-modal Semantic Localization**|Zhiqiang Yuan et.al.|[2209.06515v2](http://arxiv.org/abs/2209.06515v2)|**[link](https://github.com/xiaoyuan1996/semanticlocalizationmetrics)**|
|**2022-09-14**|**Jointly Contrastive Representation Learning on Road Network and Trajectory**|Zhenyu Mao et.al.|[2209.06389v1](http://arxiv.org/abs/2209.06389v1)|**[link](https://github.com/mzy94/jclrnt)**|
|**2022-09-13**|**CometKiwi: IST-Unbabel 2022 Submission for the Quality Estimation Shared Task**|Ricardo Rei et.al.|[2209.06243v1](http://arxiv.org/abs/2209.06243v1)|null|
|**2022-09-13**|**Space-Efficient Random Walks on Streaming Graphs**|Serafeim Papadias et.al.|[2209.06063v1](http://arxiv.org/abs/2209.06063v1)|**[link](https://github.com/spapadias/wharf)**|
|**2022-09-13**|**Streaming End-to-End Multilingual Speech Recognition with Joint Language Identification**|Chao Zhang et.al.|[2209.06058v1](http://arxiv.org/abs/2209.06058v1)|null|
|**2022-09-13**|**Don't Judge a Language Model by Its Last Layer: Contrastive Learning with Layer-Wise Attention Pooling**|Dongsuk Oh et.al.|[2209.05972v1](http://arxiv.org/abs/2209.05972v1)|**[link](https://github.com/nlpods/layerattpooler)**|
|**2022-09-12**|**Manifold Rewiring for Unlabeled Imaging**|Valentin Debarnot et.al.|[2209.05168v1](http://arxiv.org/abs/2209.05168v1)|null|
|**2022-09-14**|**Knowledge Base Question Answering: A Semantic Parsing Perspective**|Yu Gu et.al.|[2209.04994v2](http://arxiv.org/abs/2209.04994v2)|null|
|**2022-09-11**|**Inverse Image Frequency for Long-tailed Image Recognition**|Konstantinos Panagiotis Alexandridis et.al.|[2209.04861v1](http://arxiv.org/abs/2209.04861v1)|**[link](https://github.com/kostas1515/iif)**|

<p align=right>(<a href=#Updated-on-20220917>back to top</a>)</p>

## adaptor

|Publish Date|Title|Authors|PDF|Code|
|---|---|---|---|---|
|**2022-09-15**|**OmniVL:One Foundation Model for Image-Language and Video-Language Tasks**|Junke Wang et.al.|[2209.07526v1](http://arxiv.org/abs/2209.07526v1)|null|
|**2022-08-17**|**Learning with Local Gradients at the Edge**|Michael Lomnitz et.al.|[2208.08503v1](http://arxiv.org/abs/2208.08503v1)|null|
|**2022-08-15**|**MM-GNN: Mix-Moment Graph Neural Network towards Modeling Neighborhood Feature Distribution**|Wendong Bi et.al.|[2208.07012v1](http://arxiv.org/abs/2208.07012v1)|null|
|**2022-06-18**|**Camera Adaptation for Fundus-Image-Based CVD Risk Estimation**|Zhihong Lin et.al.|[2206.09202v1](http://arxiv.org/abs/2206.09202v1)|**[link](https://github.com/linzhlalala/cvd-risk-based-on-retinal-fundus-images)**|
|**2022-06-13**|**Towards Universal Sequence Representation Learning for Recommender Systems**|Yupeng Hou et.al.|[2206.05941v1](http://arxiv.org/abs/2206.05941v1)|**[link](https://github.com/rucaibox/unisrec)**|
|**2022-05-25**|**Domain Adaptation for Object Detection using SE Adaptors and Center Loss**|Sushruth Nagesh et.al.|[2205.12923v1](http://arxiv.org/abs/2205.12923v1)|**[link](https://github.com/shreyasrajesh/DA-Object-Detection)**|
|**2022-05-22**|**All Birds with One Stone: Multi-task Text Classification for Efficient Inference with One Forward Pass**|Jiaxin Huang et.al.|[2205.10744v1](http://arxiv.org/abs/2205.10744v1)|null|
|**2022-05-15**|**GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech Synthesis**|Rongjie Huang et.al.|[2205.07211v1](http://arxiv.org/abs/2205.07211v1)|null|
|**2022-04-21**|**Primary accelerometer calibration with two-axis automatic positioning stage**|Wataru Kokuyama et.al.|[2204.09212v2](http://arxiv.org/abs/2204.09212v2)|null|
|**2022-04-01**|**Universal Adaptor: Converting Mel-Spectrograms Between Different Configurations for Speech Synthesis**|Fan-Lin Wang et.al.|[2204.00170v1](http://arxiv.org/abs/2204.00170v1)|**[link](https://github.com/BogiHsu/Universal-Adaptor)**|
|**2022-06-20**|**Style-Guided Domain Adaptation for Face Presentation Attack Detection**|Young-Eun Kim et.al.|[2203.14565v2](http://arxiv.org/abs/2203.14565v2)|null|

<p align=right>(<a href=#Updated-on-20220917>back to top</a>)</p>

## object detection

|Publish Date|Title|Authors|PDF|Code|
|---|---|---|---|---|
|**2022-09-15**|**FFPA-Net: Efficient Feature Fusion with Projection Awareness for 3D Object Detection**|Chaokang Jiang et.al.|[2209.07419v1](http://arxiv.org/abs/2209.07419v1)|null|
|**2022-09-15**|**Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge**|Zhihong Chen et.al.|[2209.07118v1](http://arxiv.org/abs/2209.07118v1)|null|
|**2022-09-15**|**PROB-SLAM: Real-time Visual SLAM Based on Probabilistic Graph Optimization**|Xianwei Meng et.al.|[2209.07061v1](http://arxiv.org/abs/2209.07061v1)|null|
|**2022-09-15**|**Efficient Perception, Planning, and Control Algorithms for Vision-Based Automated Vehicles**|Der-Hau Lee et.al.|[2209.07042v1](http://arxiv.org/abs/2209.07042v1)|null|
|**2022-09-14**|**A novel illumination condition varied image dataset-Food Vision Dataset (FVD) for fair and reliable consumer acceptability predictions from food**|Swarna Sethu et.al.|[2209.06967v1](http://arxiv.org/abs/2209.06967v1)|null|
|**2022-09-14**|**Evaluating a GAN for enhancing camera simulation for robotics**|Asher Elmquist et.al.|[2209.06710v1](http://arxiv.org/abs/2209.06710v1)|null|
|**2022-09-13**|**CMR3D: Contextualized Multi-Stage Refinement for 3D Object Detection**|Dhanalaxmi Gaddam et.al.|[2209.06641v1](http://arxiv.org/abs/2209.06641v1)|null|
|**2022-09-12**|**One-Shot Doc Snippet Detection: Powering Search in Document Beyond Text**|Abhinav Java et.al.|[2209.06584v1](http://arxiv.org/abs/2209.06584v1)|null|
|**2022-09-14**|**CRAFT: Camera-Radar 3D Object Detection with Spatio-Contextual Fusion Transformer**|Youngseok Kim et.al.|[2209.06535v1](http://arxiv.org/abs/2209.06535v1)|null|
|**2022-09-14**|**Viewer-Centred Surface Completion for Unsupervised Domain Adaptation in 3D Object Detection**|Darren Tsai et.al.|[2209.06407v1](http://arxiv.org/abs/2209.06407v1)|**[link](https://github.com/darrenjkt/SEE-VCN)**|
|**2022-09-14**|**A Survey on Evolutionary Computation for Computer Vision and Image Analysis: Past, Present, and Future Trends**|Ying Bi et.al.|[2209.06399v1](http://arxiv.org/abs/2209.06399v1)|null|
|**2022-09-13**|**Computer vision based vehicle tracking as a complementary and scalable approach to RFID tagging**|Pranav Kant Gaur et.al.|[2209.05911v1](http://arxiv.org/abs/2209.05911v1)|null|
|**2022-09-13**|**PSAQ-ViT V2: Towards Accurate and General Data-Free Quantization for Vision Transformers**|Zhikai Li et.al.|[2209.05687v1](http://arxiv.org/abs/2209.05687v1)|**[link](https://github.com/zkkli/psaq-vit)**|
|**2022-09-13**|**ComplETR: Reducing the cost of annotations for object detection in dense scenes with vision transformers**|Achin Jain et.al.|[2209.05654v1](http://arxiv.org/abs/2209.05654v1)|null|
|**2022-09-12**|**CenterFormer: Center-based Transformer for 3D Object Detection**|Zixiang Zhou et.al.|[2209.05588v1](http://arxiv.org/abs/2209.05588v1)|**[link](https://github.com/tusimple/centerformer)**|

<p align=right>(<a href=#Updated-on-20220917>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/stoneyang/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/stoneyang/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/stoneyang/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/stoneyang/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/stoneyang/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/stoneyang/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/stoneyang/cv-arxiv-daily/issues

